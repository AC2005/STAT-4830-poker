---
marp: true
theme: default
paginate: true
---

# **Reasoning Model for No-Limit Holdâ€™em Poker**

## **Project Overview**
- Develop a **reasoning model** optimized for poker.
- Goal: **Maximize Expected Value (EV)** through **strategic and adaptive play**.
- Applications in **game theory, economic modeling, and negotiation**.

---

## **1. Success Metrics**
âœ… **Expected Value per Hand (EV)**  
âœ… **Win Rate vs. Opponents**  
âœ… **Unpredictability & Robustness**  
âœ… **Computational Efficiency**  

### **Constraints**
- **Computational Limits** â€“ Real-time decision-making is required.
- **Data Availability** â€“ Must gather diverse hand histories.

---

## **2. Risks and Challenges**
âš ï¸ **Data Quality** â€“ Hands may end without revealing all cards.  
âš ï¸ **Combinatorial Explosion** â€“ Over **56 billion** hand possibilities per player.  

### **Prior Work**
- **PokerBench** â€“ LLMs trained to play professional poker.  
- **PokerGPT** â€“ Lightweight solver leveraging LLMs.  
- **Pluribus** â€“ Near-GTO multiplayer strategies.  

---

## **3. Technical Approach**
ğŸ”¹ **Fine-tuning LLM** (DeepSeek R1) for action decisions.  
ğŸ”¹ **Self-Play with PyPokerEngine** for iterative training.  

ğŸ¯ **Goal:** Train model to process poker hand states & choose optimal actions.

---

## **4. Mathematical Formulation**
### **Maximize Expected Value (EV)**
$$\max \mathbb{E}[\text{Winnings per action}]$$

### **Minimize Regret**
$$\min \sum_{t=1}^{T} \bigl(\text{Best Possible Outcome} - \text{Chosen Action Outcome}\bigr)$$

âœ… Constraints: **Bankroll management, time limits, opponent inference.**

---

## **5. Algorithm Choice & Justification**
ğŸ¤” **Why a Reasoning Model?**  
âœ” More **efficient** and **adaptive** than GTO solvers.  
âœ” Avoids excessive **computation per hand**.  
âœ” Can generalize across diverse **poker hands & scenarios**.  

---

## **6. Implementation Strategy**
### **Using PyTorch**
1ï¸âƒ£ **Fine-Tuning Model** â€“ Train with pot odds, hand strength, & opponent tendencies.  
2ï¸âƒ£ **PyPokerEngine Integration** â€“ Simulate hands and update parameters via reward signals.  

---

## **7. Validation Methods**
ğŸ” **Comparisons to GTO Strategies**  
ğŸƒ **Testing Against Bots & Humans**  
ğŸ“Š **Measuring Win Rate & Decision Accuracy**  

---

## **8. Resource Requirements**
ğŸ’» **GPU/Cloud Compute** for model training.  
ğŸ“‚ **Historical Hand Histories** for training data.  
â³ **Time & Budget** â€“ Need efficient real-time inference.  

---

## **9. Initial Results**
âŒ **GPT-2: 0% Accuracy** â€“ Could not output strategic actions.  
âœ” **Larger Models (GPT-4, Reasoning Models)** performed significantly better.  

### **Performance Metrics**
- âœ… **Win Rates vs. Baselines**
- âœ… **Decision Accuracy (vs. GTO strategies)**
- âœ… **Consistency Across Game Scenarios**

---

## **10. Current Limitations**
ğŸš§ **Insufficient Compute** â€“ More powerful models needed.  
ğŸš§ **Unconstrained LLM Output** â€“ Must restrict responses to concise actions.  

---

## **11. Next Steps**
ğŸ¯ **Restrict LLM Output** â€“ Limit to "check", "fold", "raise 20", etc.  
ğŸ¯ **Explore More Powerful Models** â€“ DeepSeek-R1, GPT-4, or domain-specific models.  
ğŸ¯ **Enhance Data Pipeline** â€“ Annotate and systematically train with additional plays.  
ğŸ¯ **Optimize Efficiency** â€“ Reduce computational load in training & inference.  
ğŸ¯ **Investigate Modern RL Techniques** â€“ Explore **PPO (Proximal Policy Optimization)** & **GRPO (General Reinforcement Policy Optimization)** for training.  
ğŸ¯ **Conduct Literature Review** â€“ Study **reasoning models** like **TinyZero** and **DeepSeek** for potential adaptation.  
ğŸ¯ **Run & Fine-Tune TinyZero Locally** â€“ Load **TinyZero** and apply **RL fine-tuning** using our existing dataset.

---

## **12. Open Questions**
â“ **Beyond Fine-Tuning** â€“ How can RL or hierarchical reasoning improve performance?  
â“ **Constrained Output** â€“ Best method to limit model responses?  
â“ **Validating Partial Information** â€“ How to infer hidden opponent cards?  

---

## **13. Alternative Approaches**
ğŸ’¡ **Standalone Reasoning Models** (non-LLM)  
ğŸ’¡ **Pure Reinforcement Learning**  
ğŸ’¡ **Hybrid Approach** â€“ Mix of GTO solvers & RL  

---

## **14. Key Learnings**
âœ… **Small LLMs are unreliable** for structured decision-making.  
âœ… **GTO models are too computationally expensive** for real-time play.  
âœ… **Reasoning-based models show promise** for strategic adaptability.  

---

# **Final Thoughts**
ğŸš€ **By leveraging reasoning models, self-play, and structured training, we aim to create a powerful No-Limit Holdâ€™em AI.**  
ğŸµ **Project Playlist:** [Spotify Link](https://open.spotify.com/playlist/4hIpmIOvxOfYcRQ1p4Ct6H?si=06e5e62d6fbb4c13)  

---
